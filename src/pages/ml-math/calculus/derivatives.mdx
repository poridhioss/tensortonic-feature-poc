---
layout: ../../../layouts/DocsLayout.astro
title: Derivatives
---

import Callout from '../../../components/Callout.astro';
import Formula from '../../../components/Formula.astro';

# Derivatives

Derivatives are the foundation of optimization in machine learning. They tell us how a function changes as its input changes, which is essential for training models.

## The Derivative

<Callout type="definition" title="Definition: Derivative">
The **derivative** of a function $f(x)$ at point $x$ measures the instantaneous rate of change:
$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$
</Callout>

## Common Derivatives

| Function | Derivative |
|----------|------------|
| $x^n$ | $nx^{n-1}$ |
| $e^x$ | $e^x$ |
| $\ln(x)$ | $1/x$ |
| $\sin(x)$ | $\cos(x)$ |
| $\cos(x)$ | $-\sin(x)$ |

## Rules

<Formula name="Sum Rule">
$$\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$$
</Formula>

<Formula name="Product Rule">
$$\frac{d}{dx}[f(x)g(x)] = f'(x)g(x) + f(x)g'(x)$$
</Formula>

<Formula name="Chain Rule">
$$\frac{d}{dx}[f(g(x))] = f'(g(x)) \cdot g'(x)$$
</Formula>

<Callout type="tip" title="ML Connection">
The chain rule is essential for **backpropagation** in neural networks, allowing gradients to flow through layers.
</Callout>

---

**Next:** [Gradients](/ml-math/calculus/gradients)
